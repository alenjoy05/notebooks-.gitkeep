{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007b7303",
   "metadata": {},
   "source": [
    "\n",
    "# House Pricing — PyTorch DNN Regression (Local CSV)\n",
    "\n",
    "**Dataset:** `/mnt/data/House_Pricing.csv` (uploaded in this chat)  \n",
    "\n",
    "This notebook:\n",
    "1. Loads the dataset from the local file  \n",
    "2. Cleans & preprocesses (missing values, numeric/categorical handling, scaling, one-hot encoding)  \n",
    "3. Splits into Train/Validation/Test  \n",
    "4. Builds and trains a PyTorch Deep Neural Network with BatchNorm & Dropout  \n",
    "5. Evaluates on the Test set with **MAE**, **MSE**, **RMSE**, **R²**  \n",
    "6. Plots learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed on Colab:\n",
    "# !pip install -q torch torchvision torchaudio scikit-learn pandas numpy matplotlib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Config\n",
    "LOCAL_PATH = \"/mnt/data/House_Pricing.csv\"\n",
    "TARGET_COL = \"price\"        # Change if target is named differently\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE_WITHIN_TRAIN = 0.1765  # ~15% of full as validation\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 60                 # Reasonable runtime; adjust as needed\n",
    "PATIENCE = 12               # Early stopping patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "assert os.path.exists(LOCAL_PATH), f\"File not found: {LOCAL_PATH}\"\n",
    "df = pd.read_csv(LOCAL_PATH)\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c57068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick audit\n",
    "print(\"Missing values per column:\\n\", df.isna().sum().sort_values(ascending=False))\n",
    "display(df.describe(include='all').transpose())\n",
    "\n",
    "# Ensure target exists\n",
    "assert TARGET_COL in df.columns, f\"TARGET_COL='{TARGET_COL}' not found in columns: {list(df.columns)}\"\n",
    "\n",
    "# Separate features/target\n",
    "y = df[TARGET_COL].astype(float)\n",
    "\n",
    "# Drop ID-like columns to avoid leakage\n",
    "id_like = [c for c in df.columns if c.lower() in {\"id\", \"index\"}]\n",
    "X = df.drop(columns=[TARGET_COL] + id_like, errors=\"ignore\")\n",
    "\n",
    "# Identify numeric & categorical columns\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns ({len(num_cols)}):\", num_cols[:12], \"...\" if len(num_cols) > 12 else \"\")\n",
    "print(f\"Categorical columns ({len(cat_cols)}):\", cat_cols[:12], \"...\" if len(cat_cols) > 12 else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a60108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/Val/Test split 70/15/15\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=VAL_SIZE_WITHIN_TRAIN, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Splits:\")\n",
    "print(\"  Train:\", X_train.shape, y_train.shape)\n",
    "print(\"  Val  :\", X_val.shape, y_val.shape)\n",
    "print(\"  Test :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing: numeric (median -> scale), categorical (mode -> one-hot)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train_prep = preprocess.fit_transform(X_train)\n",
    "X_val_prep   = preprocess.transform(X_val)\n",
    "X_test_prep  = preprocess.transform(X_test)\n",
    "\n",
    "input_dim = X_train_prep.shape[1]\n",
    "print(\"Input dimension after preprocessing:\", input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PyTorch dataset/dataloader\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(np.asarray(y), dtype=torch.float32).view(-1, 1)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = TabularDataset(X_train_prep, y_train)\n",
    "val_ds   = TabularDataset(X_val_prep,   y_val)\n",
    "test_ds  = TabularDataset(X_test_prep,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model\n",
    "class RegressionDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RegressionDNN(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop with early stopping\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "train_losses, val_losses = [], []\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "    train_loss = running / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    running_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            vpred = model(xb)\n",
    "            vloss = criterion(vpred, yb)\n",
    "            running_val += vloss.item() * xb.size(0)\n",
    "    val_loss = running_val / len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss + 1e-8 < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f}\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch}. Best Val MSE: {best_val:.6f}\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning curves\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train MSE\")\n",
    "plt.plot(val_losses, label=\"Val MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ccab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).cpu().numpy().ravel()\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(yb.numpy().ravel().tolist())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"=== Test Metrics ===\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "print(f\"MSE : {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R^2 : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model weights (optional)\n",
    "torch.save(model.state_dict(), \"house_price_dnn_local.pt\")\n",
    "print(\"Saved weights -> house_price_dnn_local.pt\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
