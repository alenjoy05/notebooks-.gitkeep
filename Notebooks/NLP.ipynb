{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"spam.csv\",encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "uL9TAFhagQoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "vHxTC1jOg-UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "w7hF5zP5hErW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Punctuation"
      ],
      "metadata": {
        "id": "-x7I-NTEjh0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "Hgkua4ldh8Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "398ebffa"
      },
      "source": [
        "punctuationfree = ' '\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree = ''.join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "agkSLHUqjbYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercase Conversion"
      ],
      "metadata": {
        "id": "stKzacu1jnE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['v2_no_punctuation'] = data['v2'].apply(lambda x: remove_punctuation(x))\n",
        "data['msg_lower'] = data['v2_no_punctuation'].apply(lambda x:x.lower())"
      ],
      "metadata": {
        "id": "N4cex4ujjrJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "NSjjmqB1j-AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenization"
      ],
      "metadata": {
        "id": "T4d--1hdkIMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "def tokenization(text):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  return words"
      ],
      "metadata": {
        "id": "iHFfSIlrkPBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "ih949NyFkfWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['msg_tokenzied'] = data['msg_lower'].apply(lambda x: tokenization(x))"
      ],
      "metadata": {
        "id": "GTiZX4swkm4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "nj88xqrVk4Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# removal of stop words"
      ],
      "metadata": {
        "id": "tu2nHtbXlfRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Ku0JmJGUlh-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "Qjqu6-xll5f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  output = [i for i in text if i not in stopwords]\n",
        "  return output"
      ],
      "metadata": {
        "id": "LY4S3EeMmCfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['no_stopwords'] = data['msg_tokenzied'].apply(lambda x:remove_stopwords(x))"
      ],
      "metadata": {
        "id": "5nArVmkemPaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "OiUbFKKfmbfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(stopwords)"
      ],
      "metadata": {
        "id": "8zU1qhkdm1dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "2R_rIGHUnaaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "UC8CbXX_ncW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "  stem_text = [ps.stem(word) for word in text]\n",
        "  return stem_text"
      ],
      "metadata": {
        "id": "IpCNMbghnl0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['msg_stemmed'] = data['no_stopwords'].apply(lambda x:stemming(x))"
      ],
      "metadata": {
        "id": "vo_8pxdfn8rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "lAb6T0AooJAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "iaQvIgjJoMVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ZkWNOp4_oL0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "_i1VY2y1om0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def lemma(text):\n",
        "  lemma_text = [wordnet_lemmatizer.lemmatize(x)for x in text]\n",
        "  return lemma_text"
      ],
      "metadata": {
        "id": "47BPiPXToq-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['msg_lemmatized'] = data['msg_stemmed'].apply(lambda x:lemma(x))"
      ],
      "metadata": {
        "id": "boJwet_fpcps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "1mB6Nlmeprki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_txt'] = data['msg_lemmatized'].apply(lambda x:\" \".join(x))"
      ],
      "metadata": {
        "id": "KoLuplwYuQO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(1)"
      ],
      "metadata": {
        "id": "uN8NagJiugD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "cout_vec = count_vectorizer.fit_transform(data['clean_txt'])"
      ],
      "metadata": {
        "id": "g3GXBPJwuoIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer.vocabulary"
      ],
      "metadata": {
        "id": "0HasffeEvcaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cout_vec.toarray()"
      ],
      "metadata": {
        "id": "yezaT_-rvSqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BoW Example"
      ],
      "metadata": {
        "id": "46-cD69aq9F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents=[\"This is the first document.\",\n",
        "           \"This document is the second document.\",\n",
        "           \"And this is the third \"]\n",
        "\n",
        "vocabulary = set()\n",
        "for document in documents:\n",
        "  for word in document.lower().split():\n",
        "    vocabulary.add(word)\n",
        "\n",
        "bow_representation = []\n",
        "for document in documents:\n",
        "  document_bow = []\n",
        "  for word in vocabulary:\n",
        "    if word in document.lower().split():\n",
        "      document_bow.append(1)\n",
        "    else:\n",
        "      bow_representation.append(document_bow)\n",
        "\n",
        "print(\"vocabulary:\",list(vocabulary))\n",
        "for i, doc_bow in enumerate(bow_representation):\n",
        "  print(f\"Document(i+1):\",doc_bow)\n"
      ],
      "metadata": {
        "id": "c1KL4YUNq8W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "139cfbea"
      },
      "source": [
        "# Task\n",
        "Apply a classical machine learning model and deep learning model to NLP Spam/non-spam example. Train and evaluate the models. Select the model with best accuracy. Convert all preprocessing steps to a single function. Demonstrate the model output for a sample test input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5adfc7b"
      },
      "source": [
        "# Separate the features (clean text) and the target variable (spam/ham). Encode the target variable.\n",
        "# Split the data into training and testing sets.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data['clean_txt']\n",
        "y = data['v1']\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd81152c"
      },
      "source": [
        "# Train a classical machine learning model using the Count Vectorizer output as features.\n",
        "# This involves initializing a CountVectorizer, fitting and transforming the data, initializing a Naive Bayes model, and training the model.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_train_count = count_vectorizer.fit_transform(X_train)\n",
        "X_test_count = count_vectorizer.transform(X_test)\n",
        "\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(X_train_count, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ace21a5c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred_nb = model_nb.predict(X_test_count)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "precision = precision_score(y_test, y_pred_nb)\n",
        "recall = recall_score(y_test, y_pred_nb)\n",
        "f1 = f1_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fb9c754"
      },
      "source": [
        "# Build and train a simple deep learning model for text classification using TensorFlow/Keras.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Initialize Tokenizer and fit on training data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\") # Limit vocabulary size and handle out-of-vocabulary words\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 100 # Define maximum sequence length\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Build the deep learning model\n",
        "model_dl = Sequential()\n",
        "model_dl.add(Embedding(input_dim=5000, output_dim=16, input_length=max_length)) # Embedding layer\n",
        "model_dl.add(LSTM(32)) # LSTM layer\n",
        "model_dl.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model_dl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_dl.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2UkSJ8y0ZxB"
      },
      "source": [
        "loss, accuracy_dl = model_dl.evaluate(X_test_padded, y_test, verbose=0)\n",
        "print(f\"Deep Learning Model Accuracy: {accuracy_dl:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2cc0681"
      },
      "source": [
        "#  Compare the performance metrics of the Naive Bayes model and the deep learning model.\n",
        "\n",
        "print(\"Classical Machine Learning Model (Naive Bayes) Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(\"\\nDeep Learning Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_dl:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873e4c61"
      },
      "source": [
        "# Define a function that encapsulates all the preprocessing steps.\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation\n",
        "    punctuationfree = ''.join([i for i in text if i not in string.punctuation])\n",
        "\n",
        "    # Lowercase Conversion\n",
        "    msg_lower = punctuationfree.lower()\n",
        "\n",
        "    # Word Tokenization\n",
        "    words = nltk.word_tokenize(msg_lower)\n",
        "\n",
        "    # Removal of stop words\n",
        "    output = [i for i in words if i not in stopwords_english]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemma_text = [wordnet_lemmatizer.lemmatize(x) for x in output]\n",
        "\n",
        "    # Join back into a string\n",
        "    clean_text = \" \".join(lemma_text)\n",
        "\n",
        "    return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e7a2dec"
      },
      "source": [
        "sample_text = \"Hello! This is a test message, with some stop words and punctuation.\"\n",
        "processed_sample_text = preprocess_text(sample_text)\n",
        "print(processed_sample_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ad136d0"
      },
      "source": [
        "sample_test_input = \"Claim your free prize now! Click here.\"\n",
        "processed_sample_input = preprocess_text(sample_test_input)\n",
        "sample_input_count = count_vectorizer.transform([processed_sample_input])\n",
        "prediction = model_nb.predict(sample_input_count)\n",
        "\n",
        "print(f\"Original Text: {sample_test_input}\")\n",
        "print(f\"Predicted Class: {'spam' if prediction[0] == 1 else 'ham'}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}